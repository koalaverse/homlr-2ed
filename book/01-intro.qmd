# Introduction to Machine Learning

```{r}
#| label: setup
#| include: FALSE
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  cache = TRUE
)
```

Machine learning (ML) continues to grow in importance for many organizations across
nearly all domains. Some example applications of ML in practice include:

* Predicting the likelihood of a patient returning to the hospital (_readmission_) within
30 days of discharge.
* Segmenting customers based on common attributes or purchasing behavior for targeted
marketing.
* Predicting coupon redemption rates for a given marketing campaign.
* Predicting customer churn so an organization can perform preventative intervention.
* And many more!

In essence, these tasks all seek to learn and draw inferences from patterns in data.  To address each scenario, we can
use a given set of _features_\index{features} to train an algorithm and extract insights.
These algorithms, or _learners_\index{learners}, can be classified according to the amount
and type of supervision needed during training.  The two main groups this book focuses on
are: ___supervised learners___ which construct predictive models, and ___unsupervised
learners___ which build descriptive models. Which type you will need to use depends on
the learning task you hope to accomplish.

## Supervised learning

::: {.content-hidden unless-format="html"}

A ___predictive model___\index{predictive model} is used for tasks that involve the
prediction of a given output (or target) using other variables (or features) in the data
set. Or, as stated by @apm [p. 2], predictive modeling is "...the process of developing
a mathematical tool or model that generates an accurate prediction."  The learning algorithm
in a predictive model attempts to discover and model the relationships among the
<font color="red">target</font> variable (the variable being predicted) and the other
<font color="blue">features</font> (aka predictor variables). Examples of predictive modeling
include:

* using <font color="blue">customer attributes</font> to predict the probability of the
<font color="red">customer churning</font> in the next 6 weeks;
* using <font color="blue">home attributes</font> to predict the <font color="red">sales price</font>;
* using <font color="blue">employee attributes</font> to predict the likelihood of
<font color="red">attrition</font>;
* using <font color="blue">patient attributes</font> and symptoms to predict the risk of
<font color="red">readmission</font>;
* using <font color="blue">production attributes</font> to predict <font color="red">time
to market</font>.

:::

::: {.content-hidden unless-format="pdf"}

A ___predictive model___\index{predictive model} is used for tasks that involve the
prediction of a given output (or target) using other variables (or features) in the data
set. Or, as stated by @apm [p. 2], predictive modeling is "...the process of developing
a mathematical tool or model that generates an accurate prediction."  The learning algorithm
in a predictive model attempts to discover and model the relationships among the
target variable (the variable being predicted) and the other features (aka predictor
variables). Examples of predictive modeling include:

* using customer attributes to predict the probability of the customer churning in the
next 6 weeks;
* using home attributes to predict the sales price;
* using employee attributes to predict the likelihood of attrition;
* using patient attributes and symptoms to predict the risk of readmission;
* using production attributes to predict time to market.

:::

Each of these examples has a defined learning task; they each intend to use attributes
($X$) to predict an outcome measurement ($Y$).

::: {.callout-note}
Throughout this text we'll use various terms interchangeably for

* $X$: "predictor variable", "independent variable", "attribute", "feature", "predictor"
* $Y$: "target variable", "dependent variable", "response", "outcome measurement"
:::

The predictive modeling examples above describe what is known as _supervised learning_\index{supervised learning}.
The supervision refers to the fact that the target values provide a supervisory role,
which indicates to the learner the task it needs to learn. Specifically, given a set of
data, the learning algorithm attempts to optimize a function (the algorithmic steps) to
find the combination of feature values that results in a predicted value that is as close
to the actual target output as possible.

::: {.callout-note}
In supervised learning, the training data you feed the algorithm includes the target
values.  Consequently, the solutions can be used to help _supervise_ the training process
to find the optimal algorithm parameters.
:::

Most supervised learning problems can be bucketed into one of two general categories,
_regression_\index{regression} or _classification_\index{classification}, depending on the type of response variable. We'll
briefly discuss both cases over the next two sections.

### Regression problems

```{r}
#| label: fig-intro-regression-problem-data
#| echo: FALSE

df <- AmesHousing::make_ames()
x <- matrix(sort(df$Gr_Liv_Area)[floor(seq(1, nrow(df), length.out = 15))], 15, 1)
y <- matrix(sort(df$Year_Built)[floor(seq(1, nrow(df), length.out = 15))], 1, 15)
z <- 25051 + 3505*(log(x^.9) %*% log(y)) - 5*as.vector(x)
c <- matrix(c(.92, .95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, .95), 1, 15)
z <- sweep(z, MARGIN = 2, c, `*`)
```

::: {.content-hidden unless-format="html"}

When the objective is to predict a quantitative outcome, we generally refer
to this as a _regression problem_ (not to be confused with linear regression modeling, which is a special case).
Regression problems revolve around numeric output where both order and distance matters (e.g., sales or a discrete count, like the number of bike rentals in a given day). In the
examples above, predicting home sale prices and time to market reflect a regression problem
because the output is ordered and values closer to each other are closer in nature (e.g., the closer two sale prices are to each other the more similar the homes are in terms of sale value).
@fig-intro-regression-problem-html shows a regression model's predicted sale price of homes in Ames, Iowa (from 2006--2010) as a
function of two attributes: year built and total square footage. Depending on the
combination of these two features, the expected home sales price could fall anywhere along
the surface. We'll learn various ways of building such a model in Part II of this book (TODO: cross-reference "Part II").

```{r}
#| label: fig-intro-regression-problem-html
#| echo: FALSE
#| fig-cap: "Average home sales price as a function of year built and total square footage."
#| fig-height: 3
#| fig-width: 3
library(plotly)
plot_ly(x = as.vector(x), y = as.vector(y), z = z, showscale = FALSE) %>%
    add_surface() %>%
    layout(
        scene = list(
            xaxis = list(title = "Square footage"),
            yaxis = list(title = "Year built"),
            zaxis = list(title = "Sale price (dollars)")
        )
    )
```
:::

::: {.content-hidden unless-format="pdf"}

When the objective is to predict a quantitative outcome, we generally refer
to this as a _regression problem_ (not to be confused with linear regression modeling, which is a special case).
Regression problems revolve around numeric output where both order and distance matters (e.g., sales or a discrete count, like the number of bike rentals in a given day). In the
examples above, predicting home sale prices and time to market reflect a regression problem
because the output is ordered and values closer to each other are closer in nature (e.g., the closer two sale prices are to each other the more similar the homes are in terms of sale value).
@fig-intro-regression-problem-pdf shows a regression model's predicted sale price of homes in Ames, Iowa (from 2006--2010) as a
function of two attributes: year built and total square footage. Depending on the
combination of these two features, the expected home sales price could fall anywhere along
the surface. We'll learn various ways of building such a model in Part II of this book (TODO: cross-reference "Part II").

```{r}
#| label: fig-intro-regression-problem-pdf
#| echo: FALSE
#| fig-cap: "Average home sales price as a function of year built and total square footage."
#| fig-height: 3
#| fig-width: 3
# code for 3D print version
par(mar = c(0.1, 0.1, 0.1, 0.1))  # remove extra white space
persp(
  x = x,
  y = y,
  z = z,
  xlab = "Square footage",
  ylab = "Year built",
  zlab = "Sale price",
  theta = -45,
  phi = 25,
  col = viridis::viridis(100)
)
```
:::

### Classification problems

When the objective of our supervised learning is to predict a qualitative (or categorical outcome), we
refer to this generally as a _classification problem_.  Classification problems most commonly
revolve around predicting a binary or multinomial response measure such as:

* Did a customer redeem a coupon (coded as yes/no or 1/0)?
* Did a customer churn (coded as yes/no or 1/0)?
* Did a customer click on our online ad (coded as yes/no or 1/0)?
* Classifying customer reviews:
    * Binary: positive vs. negative.
    * Multinomial: extremely negative to extremely positive on a 0--5 Likert scale.

```{r}
#| label: fig-intro-classification-problem
#| echo: FALSE
#| fig-cap: "Classification problem modeling 'Yes'/'No' response based on three features."
#| output-height: "50%"
#| output-width: "50%"

# code to create graphic
library(DiagrammeR)
 grViz("

   digraph boxes_and_circles {
     node [shape = circle]
     x1; x2; x3;

     node [shape = box]
     Model;

     node [shape = triangle]
     Yes; No;

     x1->Model; x2->Model; x3->Model; Model->No; Model->Yes;
 }")
```

However, when we apply ML models to classification problems, rather than
predict a particular class (i.e., "yes" or "no"), we often want to predict the _conditional probability_
of a particular class (i.e., yes: 0.65, no: 0.35).^[Conditional on the set of input feature values.]  By default, the class with the highest
predicted probability becomes the predicted class (called a classification).  Consequently, even though we
classify it as a classification problem (pun intended), we're often still predicting a numeric output (i.e., a probability).
However, the nature of the target variable is what makes it a classification problem. **TODO:** Reference Frank Harrell's discussion on classification vs. prediction at https://www.fharrell.com/post/classification/ (and just clarify that we knowingly abuse the term for brevity).

Although there are ML algorithms that can be applied to regression problems
but not classification and vice versa, most of the supervised learning algorithms we'll cover
in this book can be applied to both. These algorithms have become some of the most popular machine
learning models in recent years (often driven by their availability in both proprietary and open-source software).

### Knowledge check

::: {.callout-caution icon=false}
Identify the features, response variable, and the type of supervised model required for
the following tasks:

* There is an online retailer that wants to predict whether you will click on a certain
featured product given your demographics, the current products in your online basket, and
the time since your previous purchase.
* A bank wants to use a customers historical data such as the number of loans they've had,
the time it took to payoff those loans, previous loan defaults, the number of new loans
within the past two years, along with the customers income and level of education to
determine if they should issue them a new loan for a car.
* If the bank above does issue a new loan, they want to use the same information to
determine the interest rate of the new loan issued.
* To better plan incoming and outgoing flights, an airline wants to use flight information
such as scheduled flight time, day/month of year, number of passengers, airport departing
from, airport arriving to, distance to travel, and weather warnings to determine if a
flight will be delayed.
* What if the above airline wants to use the same information to predict the number of
minutes a flight will arrive late or early?
:::

## Unsupervised learning

_Unsupervised learning_\index{unsupervised learning}, in contrast to supervised
learning, includes a set of statistical tools to better understand and describe your data,
but performs the analysis without a target variable.  In essence, unsupervised learning
is concerned with identifying groups in a data set. The groups may be defined by the rows
(i.e., *clustering*) or the columns (i.e., *dimension reduction*); however, the motive in
each case is quite different.

The goal of _clustering_\index{clustering} is to segment observations into similar
groups based on the observed variables; for example, dividing consumers into different
homogeneous groups, a process known as market segmentation.  In _dimension
reduction_\index{dimension reduction}, we are often concerned with reducing the number
of variables in a data set. For example, classical linear regression models break down
in the presence of highly correlated features, a situation known as *multicollinearity*.^[To be fair, and as we'll see later in the book, the interpretation of most fitted ML models becomes problematic in the presence of correlated or (otherwise dependent) fetures.] Some dimension reduction techniques can
be used to reduce the feature set to a potentially smaller set of uncorrelated variables.
Such a reduced feature set is often used as input to downstream supervised learning models
(e.g., principal component regression).

Unsupervised learning is often performed as part of an exploratory data analysis (EDA).
However, the exercise tends to be more subjective, and there is no simple goal for the
analysis, such as prediction of a response. Furthermore, it can be hard to assess the
quality of results obtained from unsupervised learning methods. The reason for this is
simple. If we fit a predictive model using a supervised learning technique (e.g., linear
regression), then it is possible to check our work by seeing how well our model predicts
the response $y$ on new observations not used in fitting the model. However, in unsupervised
learning, there's no way to check our work because we don’t know the true answer---the
problem is unsupervised!

Despite its subjectivity, the importance of unsupervised learning should not be overlooked
and such techniques are often used in organizations to:

- Divide consumers into different homogeneous groups so that tailored marketing strategies
can be developed and deployed for each segment.
- Identify groups of online shoppers with similar browsing and purchase histories, as well
as items that are of particular interest to the shoppers within each group. Then an
individual shopper can be preferentially shown the items in which he or she is particularly
likely to be interested, based on the purchase histories of similar shoppers.
- Identify products that have similar purchasing behavior so that managers can manage them
as product groups.

These questions, and many more, can be addressed with unsupervised learning.  Moreover,
the outputs of unsupervised learning models can be used as inputs to downstream supervised
learning models.

### Knowledge check

::: {.callout-caution icon=false}
Identify the type of unsupervised model required for the following tasks:

* Say you have a YouTube channel. You may have a lot of data about the subscribers of
your channel. What if you want to use that data to detect groups of similar subscribers?
* Say you'd like to group Ohio counties together based on the demographics of their
residents.
* A retailer has collected hundreds of attributes about all their customers; however,
many of those features are highly correlated. They'd like to reduce the number of features
down by combining all those highly correlated features into groups.
:::

## Machine Learning in `r fontawesome::fa("r-project")`

Historically, the R ecosystem provides a wide variety of ML algorithm implementations.
This has its benefits; however, this also has drawbacks as it requires the users to learn
many different formula interfaces and syntax nuances.

More recently, development on a group of packages called [**Tidymodels**](https://www.tidymodels.org/)
has helped to make implementation easier. The **tidymodels** collection allows you to perform
discrete parts of the ML workflow with discrete packages:

- [rsample](https://rsample.tidymodels.org/) for data splitting and resampling
- [recipes](https://recipes.tidymodels.org/) for data pre-processing and feature engineering
- [parsnip](https://parsnip.tidymodels.org/) for applying algorithms
- [tune](https://tune.tidymodels.org/) for hyperparameter tuning
- [yardstick](https://yardstick.tidymodels.org/) for measuring model performance
- and several others!

Throughout this book you'll be exposed to several of these packages. Go ahead and make
sure you have the following packages installed.

::: {.callout-note}
The **tidymodels** package is a meta package, or a package of packages, that will install
several packages that exist in the **tidymodels** ecosystem.
:::

```{r}
#| label: packages-to-install
#| eval: FALSE

# data wrangling
install.packages(c("here", "tidyverse"))

# modeling
install.packages("tidymodels")

# model interpretability
install.packages(c("pdp", "vip"))
```

```{r}
#| label: load-tidymodels
#| message: TRUE
packageVersion("tidymodels")

library(tidymodels)
```

### Knowledge check

::: {.callout-caution icon=false}
Check out the Tidymodels website: https://www.tidymodels.org/. Identify which packages
can be used for:

1. Efficiently splitting your data
2. Optimizing hyperparameters
3. Measuring the effectiveness of your model
4. Working with correlation matrices
:::

## Roadmap

The goal of this book is to provide effective methods and tools for uncovering relevant and useful
patterns in your data by using R's ML stack. We begin by providing an overview of the ML
modeling process and discussing fundamental concepts that will carry through the rest of
the book. These include feature engineering, data splitting, model validation and tuning,
and assessing model performance. These concepts will be discussed more thoroughly in Chapters ...

::: {.callout-warning}
# TODO
Fill out roadmap as we progress
:::


## Data sets {#sec-data-intro}

::: {.callout-warning}
# TODO
Revisit as we progress
:::

## Exercises

1. Identify four real-life applications of supervised and unsupervised problems.
   - Explain what makes these problems supervised versus unsupervised.
   - For each problem identify the target variable (if applicable) and potential
     features.

2. Identify and contrast a regression problem with a classification problem.
   - What is the target variable in each problem and why would being able to
     accurately predict this target be beneficial to society?
   - What are potential features and where could you collect this information?
   - What is determining if the problem is a regression or a classification
     problem?

3. Identify three open source data sets suitable for ML (e.g.,
   https://bit.ly/35wKu5c).
   - Explain the type of ML models that could be constructed from
     the data (e.g., supervised versus unsupervised and regression versus
     classification).
   - What are the dimensions of the data?
   - Is there a code book that explains who collected the data, why it was
     originally collected, and what each variable represents?
   - If the data set is suitable for supervised learning, which variable(s) could
     be considered as a useful target? Which variable(s) could be considered as
     features?

4. Identify examples of misuse of ML in society. What was the
   ethical concern?
